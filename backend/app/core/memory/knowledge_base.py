import os
import re
import requests
import subprocess
import json
import time
import sys
from pathlib import Path
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Smolagents imports for proper message handling
from smolagents import ChatMessage, MessageRole

# --- Knowledge Base System ---
class KnowledgeBase:
    """Knowledge base system - store and retrieve successful thinking templates"""

    def __init__(self, gemini_model=None):
        self.templates = []
        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        self.template_vectors = None
        self.knowledge_file = Path("data/outputs/agent_knowledge_base.json")
        self.gemini_model = gemini_model  # æ·»åŠ  Gemini æ¨¡å‹æ”¯æŒ

        # Load existing knowledge base
        self.load_knowledge_base()

    def summarize_reasoning_process(self, question_text, detailed_reasoning, correct_answer):
        """ä½¿ç”¨LLMæ€»ç»“æ¨ç†è¿‡ç¨‹çš„å…³é”®æ­¥éª¤"""
        summarization_prompt = f"""Please summarize the key reasoning steps from the following detailed analysis.
Focus on the essential logical steps and principles that led to the successful solution, without revealing the specific answer.

Task/Question: {question_text}

Detailed Reasoning Process:
{detailed_reasoning}

Please provide a concise summary of 4-5 key reasoning principles and methodological approaches that were crucial for solving this type of problem. Do not include the final answer.

Key Reasoning Summary:"""

        try:
            print("ğŸ§  è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†æ€»ç»“...")

            # Use correct message format for smolagents
            response = self.gemini_model([{"role": "user", "content": summarization_prompt}])

            # Handle different response formats
            if hasattr(response, 'content'):
                summary = response.content
            elif isinstance(response, dict) and 'content' in response:
                summary = response['content']
            elif isinstance(response, str):
                summary = response
            else:
                summary = str(response)

            # Clean and validate the summary
            if summary and isinstance(summary, str):
                summary = summary.strip()
                # Remove common non-informative starts
                if summary.lower().startswith(('key reasoning summary:', 'summary:', 'the key reasoning')):
                    lines = summary.split('\n')
                    summary = '\n'.join(lines[1:]).strip() if len(lines) > 1 else summary

                # Ensure it's informative (not just generic text)
                if len(summary) > 50 and 'systematic analysis' not in summary.lower():
                    # Limit length
                    if len(summary) > 800:
                        summary = summary[:800] + "..."
                    print(f"âœ… Successfully generated reasoning summary: {summary[:100]}...")
                    return summary
                else:
                    print(f"âš ï¸ Model-generated summary too generic or short: {summary[:100]}")
                    return self._generate_manual_summary(question_text, detailed_reasoning)
            else:
                print(f"âš ï¸ æ„å¤–çš„æ¨¡å‹å“åº”ç±»å‹: {type(summary)}")
                return self._generate_manual_summary(question_text, detailed_reasoning)

        except Exception as e:
            print(f"âŒ Reasoning summary failed: {str(e)}")
            import traceback
            print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return self._generate_manual_summary(question_text, detailed_reasoning)

    def _generate_manual_summary(self, question_text, detailed_reasoning):
        """Manually generate reasoning summary as fallback"""
        try:
            # Extract some key concepts from the question and reasoning
            question_lower = question_text.lower()
            reasoning_lower = detailed_reasoning.lower() if detailed_reasoning else ""

            # Identify domain-specific approaches
            if any(term in question_lower for term in ['data', 'analysis', 'csv', 'plot', 'graph']):
                return "Applied systematic data analysis with visualization and statistical interpretation approaches."
            elif any(term in question_lower for term in ['code', 'script', 'programming', 'function']):
                return "Used systematic programming approach with modular design and error handling principles."
            elif any(term in question_lower for term in ['search', 'research', 'find', 'information']):
                return "Applied comprehensive information retrieval with source verification and synthesis methods."
            elif any(term in question_lower for term in ['biomedical', 'biology', 'medical', 'health']):
                return "Applied biomedical reasoning with evidence-based analysis and scientific methodology."
            else:
                return "Applied systematic problem-solving approach with logical reasoning and evidence-based analysis."
        except:
            return "Applied systematic problem-solving approach with methodical analysis."

    def add_template(self, task_description, thought_process, solution_outcome, domain):
        """Add successful thinking template to knowledge base (using LLM summary, not storing specific answers)"""

        # ä½¿ç”¨LLMæ€»ç»“å…³é”®æ¨ç†æ­¥éª¤
        print("ğŸ§  æ­£åœ¨æ€»ç»“æ¨ç†è¿‡ç¨‹...")
        key_reasoning = self.summarize_reasoning_process(task_description, thought_process, solution_outcome)

        template = {
            'task': task_description,
            'key_reasoning': key_reasoning,  # å­˜å‚¨æ€»ç»“çš„å…³é”®æ¨ç†æ­¥éª¤
            'domain': domain,
            'keywords': self.extract_keywords(task_description),
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        self.templates.append(template)

        # é‡æ–°è®¡ç®—å‘é‡
        self.rebuild_vectors()

        # é™åˆ¶çŸ¥è¯†åº“å¤§å°
        if len(self.templates) > 1000:
            self.templates = self.templates[-1000:]  # ä¿ç•™æœ€æ–°1000ä¸ª
            self.rebuild_vectors()

        # ä¿å­˜åˆ°æ–‡ä»¶
        self.save_knowledge_base()

        print(f"ğŸ’¾ çŸ¥è¯†åº“æ–°å¢æ¨¡æ¿ï¼ˆå·²æ€»ç»“ï¼‰ï¼Œæ€»æ•°: {len(self.templates)}")

        # è¿”å›æˆåŠŸçŠ¶æ€
        return {
            "success": True,
            "message": f"Template added successfully. Total templates: {len(self.templates)}",
            "template_id": len(self.templates) - 1
        }

    def extract_keywords(self, text):
        """æå–å…³é”®è¯ - ä½¿ç”¨ Gemini æ¨¡å‹æˆ–å›é€€åˆ°é™æ€å…³é”®è¯"""

        # å¦‚æœæœ‰ Gemini æ¨¡å‹ï¼Œä½¿ç”¨ AI è¿›è¡Œå…³é”®è¯æå–
        if self.gemini_model:
            try:
                keyword_prompt = f"""Extract 3-8 most important keywords from the following text, focusing on technical, scientific, medical, biological, data analysis, programming, and related professional terms.

Please return only keywords, separated by commas, without any other explanations or punctuation.

Text: {text}

Keywords:"""

                response = self.gemini_model([{"role": "user", "content": keyword_prompt}])

                # å¤„ç†å“åº”
                if hasattr(response, 'content'):
                    keywords_str = response.content.strip()
                elif isinstance(response, dict) and 'content' in response:
                    keywords_str = response['content'].strip()
                elif isinstance(response, str):
                    keywords_str = response.strip()
                else:
                    keywords_str = str(response).strip()

                # è§£æå…³é”®è¯
                keywords = [kw.strip().lower() for kw in keywords_str.split(',')]
                keywords = [kw for kw in keywords if kw and len(kw) > 2]  # è¿‡æ»¤ç©ºç™½å’Œå¤ªçŸ­çš„è¯

                if keywords:
                    return keywords

            except Exception as e:
                print(f"âš ï¸ Gemini å…³é”®è¯æå–å¤±è´¥ï¼Œå›é€€åˆ°é™æ€æ–¹æ³•: {str(e)}")

        # å›é€€åˆ°åŸå§‹çš„é™æ€å…³é”®è¯æ–¹æ³•
        tech_keywords = [
            # é€šç”¨æŠ€æœ¯å…³é”®è¯
            'data', 'analysis', 'code', 'script', 'programming', 'algorithm',
            'visualization', 'plot', 'chart', 'graph', 'statistics', 'model',
            'database', 'search', 'information', 'literature', 'paper',

            # ç”Ÿç‰©å­¦é€šç”¨æœ¯è¯­
            'biomedical', 'biology', 'medical', 'research', 'scientific',
            'bioinformatics', 'computational', 'experimental', 'clinical',

            # åŸºäº Biomni å·¥å…·åˆ†ç±»çš„ä¸“ä¸šæœ¯è¯­
            'molecular_biology', 'molecular', 'cell_biology', 'cellular',
            'genetics', 'genetic', 'genomics', 'genome', 'genomic',
            'biochemistry', 'biochemical', 'immunology', 'immune', 'immunological',
            'microbiology', 'microbiological', 'microbial', 'bacterial',
            'cancer_biology', 'cancer', 'oncology', 'tumor', 'malignancy',
            'pathology', 'pathological', 'disease', 'disorder',
            'pharmacology', 'pharmacological', 'drug', 'therapeutic', 'treatment',
            'physiology', 'physiological', 'function', 'metabolism',
            'systems_biology', 'systems', 'network', 'pathway',
            'synthetic_biology', 'synthetic', 'engineering', 'design',
            'bioengineering', 'biomedical_engineering', 'biotechnology',
            'biophysics', 'biophysical', 'structural', 'protein', 'enzyme',

            # å®éªŒæŠ€æœ¯å’Œæ–¹æ³•
            'sequencing', 'pcr', 'microscopy', 'imaging', 'assay',
            'chromatography', 'electrophoresis', 'blotting', 'culture',
            'transfection', 'transformation', 'cloning', 'expression',
            'purification', 'crystallization', 'spectroscopy',

            # æ•°æ®åˆ†æå’Œè®¡ç®—
            'machine_learning', 'deep_learning', 'neural_network',
            'classification', 'clustering', 'regression', 'prediction',
            'simulation', 'modeling', 'optimization', 'annotation'
        ]

        text_lower = text.lower()
        found_keywords = [kw for kw in tech_keywords if kw in text_lower]
        return found_keywords

    def rebuild_vectors(self):
        """é‡å»ºå‘é‡è¡¨ç¤º"""
        if len(self.templates) == 0:
            return

        # ç»„åˆä»»åŠ¡æ–‡æœ¬ã€å…³é”®æ¨ç†å’Œå…³é”®è¯
        texts = []
        for t in self.templates:
            # å¤„ç†æ–°æ—§æ ¼å¼å…¼å®¹æ€§
            reasoning = t.get('key_reasoning', t.get('thought_process', ''))
            text = f"{t['task']} {reasoning} {' '.join(t['keywords'])}"
            texts.append(text)

        try:
            self.template_vectors = self.vectorizer.fit_transform(texts)
        except Exception as e:
            print(f"âš ï¸ å‘é‡é‡å»ºå¤±è´¥: {str(e)}")
            self.template_vectors = None

    def retrieve_similar_templates(self, task_description, top_k=3):
        """æ£€ç´¢ç›¸ä¼¼çš„æ€ç»´æ¨¡æ¿"""
        if len(self.templates) == 0 or self.template_vectors is None:
            return []

        try:
            # å‘é‡åŒ–å½“å‰ä»»åŠ¡
            task_vector = self.vectorizer.transform([task_description])

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(task_vector, self.template_vectors).flatten()

            # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„æ¨¡æ¿
            top_indices = np.argsort(similarities)[::-1][:top_k]

            similar_templates = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    template = self.templates[idx].copy()
                    template['similarity'] = similarities[idx]
                    similar_templates.append(template)

            print(f"ğŸ” æ‰¾åˆ° {len(similar_templates)} ä¸ªç›¸ä¼¼æ¨¡æ¿")
            return similar_templates

        except Exception as e:
            print(f"âš ï¸ æ¨¡æ¿æ£€ç´¢å¤±è´¥: {str(e)}")
            return []

    def search_templates_by_keyword(self, keyword):
        """æŒ‰å…³é”®è¯æœç´¢æ¨¡æ¿"""
        matching_templates = []
        keyword_lower = keyword.lower()

        for template in self.templates:
            # åœ¨ä»»åŠ¡æè¿°ã€å…³é”®æ¨ç†å’Œå…³é”®è¯ä¸­æœç´¢
            if (keyword_lower in template['task'].lower() or
                keyword_lower in template.get('key_reasoning', '').lower() or
                keyword_lower in ' '.join(template['keywords']).lower()):
                matching_templates.append(template)

        print(f"ğŸ” å…³é”®è¯ '{keyword}' åŒ¹é…åˆ° {len(matching_templates)} ä¸ªæ¨¡æ¿")
        return matching_templates

    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.knowledge_file.parent.mkdir(parents=True, exist_ok=True)

            with open(self.knowledge_file, 'w', encoding='utf-8') as f:
                json.dump(self.templates, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {str(e)}")

    def load_knowledge_base(self):
        """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“"""
        try:
            if self.knowledge_file.exists():
                with open(self.knowledge_file, 'r', encoding='utf-8') as f:
                    self.templates = json.load(f)

                # Rebuild vectors
                self.rebuild_vectors()

                print(f"âœ… Successfully loaded knowledge base with {len(self.templates)} templates")
            else:
                print("ğŸ“š Knowledge base file does not exist, starting from blank")
        except Exception as e:
            print(f"âš ï¸ Failed to load knowledge base: {str(e)}")
            self.templates = []
